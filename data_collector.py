#!/usr/bin/env python3
"""
Aè‚¡æ•°æ®é‡‡é›†æ¨¡å— - æ‰©å±•ç‰ˆæœ¬
æ”¯æŒ5å¹´å†å²æ•°æ®ã€500åªè‚¡ç¥¨ã€é«˜å¹¶å‘ä¸‹è½½ã€ä¸¥æ ¼è´¨é‡æ§åˆ¶

é‡è¦å‡çº§:
- æ—¶é—´èŒƒå›´: 2020-2024 (5å¹´ vs åŸ2å¹´)
- è‚¡ç¥¨æ•°é‡: æ”¯æŒ500åª (vs åŸ100åª)
- å¹¶å‘ä¼˜åŒ–: max_workers=5, batch_size=20
- è´¨é‡æ§åˆ¶: æ›´ä¸¥æ ¼çš„æ•°æ®éªŒè¯å’ŒæµåŠ¨æ€§ç­›é€‰
"""

import akshare as ak
import pandas as pd
import numpy as np
import os
import time
import json
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class DataCollector:
    def __init__(self, data_dir='data', cache_dir='cache'):
        self.data_dir = Path(data_dir)
        self.cache_dir = Path(cache_dir)
        
        # åˆ›å»ºç›®å½•
        self.data_dir.mkdir(exist_ok=True)
        self.cache_dir.mkdir(exist_ok=True)
        
        # çº¿ç¨‹é”ï¼Œç”¨äºAPIé™é¢‘ (ä¼˜åŒ–ä¸ºæ›´é«˜å¹¶å‘)
        self.api_lock = threading.Lock()
        self.last_api_call = 0
        self.min_interval = 0.08  # æœ€å°APIè°ƒç”¨é—´éš”(ç§’) - ä»0.1ä¼˜åŒ–åˆ°0.08
        
        # ç¼“å­˜é…ç½®
        self.cache_file = self.cache_dir / 'download_status.json'
        self.load_cache_status()
        
        print(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def load_cache_status(self):
        """åŠ è½½ä¸‹è½½çŠ¶æ€ç¼“å­˜"""
        if self.cache_file.exists():
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                self.cache_status = json.load(f)
        else:
            self.cache_status = {}
        
        print(f"ğŸ“‹ å·²åŠ è½½ç¼“å­˜çŠ¶æ€: {len(self.cache_status)} æ¡è®°å½•")
    
    def save_cache_status(self):
        """ä¿å­˜ä¸‹è½½çŠ¶æ€ç¼“å­˜"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache_status, f, ensure_ascii=False, indent=2)
    
    def rate_limit(self):
        """APIè°ƒç”¨é™é¢‘æ§åˆ¶"""
        with self.api_lock:
            current_time = time.time()
            time_since_last = current_time - self.last_api_call
            if time_since_last < self.min_interval:
                time.sleep(self.min_interval - time_since_last)
            self.last_api_call = time.time()
    
    def download_single_stock(self, stock_code, start_date='20200101', end_date='20241231'):
        """ä¸‹è½½å•åªè‚¡ç¥¨çš„æ—¥åº¦æ•°æ®"""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{stock_code}_{start_date}_{end_date}"
        if cache_key in self.cache_status and self.cache_status[cache_key]['status'] == 'success':
            return self.load_stock_data(stock_code)
        
        try:
            # APIé™é¢‘
            self.rate_limit()
            
            # è·å–æ—¥åº¦è¡Œæƒ…æ•°æ®
            daily_data = ak.stock_zh_a_hist(
                symbol=stock_code,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust=""
            )
            
            if daily_data.empty:
                raise ValueError(f"è‚¡ç¥¨ {stock_code} æ— æ•°æ®")
            
            # æ•°æ®é¢„å¤„ç†
            daily_data = self.preprocess_daily_data(daily_data, stock_code)
            
            # ä¿å­˜æ•°æ®
            output_file = self.data_dir / f"{stock_code}.csv"
            daily_data.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # æ›´æ–°ç¼“å­˜çŠ¶æ€
            self.cache_status[cache_key] = {
                'status': 'success',
                'download_time': datetime.now().isoformat(),
                'records': len(daily_data),
                'file_path': str(output_file)
            }
            
            return daily_data
            
        except Exception as e:
            # è®°å½•å¤±è´¥çŠ¶æ€
            self.cache_status[cache_key] = {
                'status': 'failed',
                'error': str(e),
                'download_time': datetime.now().isoformat()
            }
            print(f"âŒ {stock_code} ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def preprocess_daily_data(self, df, stock_code):
        """é¢„å¤„ç†æ—¥åº¦æ•°æ®"""
        df = df.copy()
        
        # æ ‡å‡†åŒ–åˆ—å
        column_mapping = {
            'æ—¥æœŸ': 'date',
            'è‚¡ç¥¨ä»£ç ': 'code', 
            'å¼€ç›˜': 'open',
            'æ”¶ç›˜': 'close',
            'æœ€é«˜': 'high',
            'æœ€ä½': 'low',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'amount',
            'æŒ¯å¹…': 'amplitude',
            'æ¶¨è·Œå¹…': 'pct_change',
            'æ¶¨è·Œé¢': 'change',
            'æ¢æ‰‹ç‡': 'turnover'
        }
        
        df = df.rename(columns=column_mapping)
        
        # ç¡®ä¿æœ‰è‚¡ç¥¨ä»£ç 
        if 'code' not in df.columns or df['code'].isna().all():
            df['code'] = stock_code
        
        # è½¬æ¢æ•°æ®ç±»å‹
        df['date'] = pd.to_datetime(df['date'])
        
        numeric_columns = ['open', 'close', 'high', 'low', 'volume', 'amount', 
                          'amplitude', 'pct_change', 'change', 'turnover']
        
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # æŒ‰æ—¥æœŸæ’åº
        df = df.sort_values('date').reset_index(drop=True)
        
        # æ•°æ®è´¨é‡æ£€æŸ¥ (æé«˜æ ‡å‡†)
        if len(df) < 1000:  # å°‘äº1000ä¸ªäº¤æ˜“æ—¥ (5å¹´çº¦1250ä¸ªäº¤æ˜“æ—¥)
            print(f"âš ï¸  {stock_code} æ•°æ®ä¸è¶³: {len(df)} ä¸ªäº¤æ˜“æ—¥ (éœ€è¦â‰¥1000å¤©ï¼Œ5å¹´æ•°æ®)")
        
        return df
    
    def load_stock_data(self, stock_code):
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½è‚¡ç¥¨æ•°æ®"""
        file_path = self.data_dir / f"{stock_code}.csv"
        if file_path.exists():
            try:
                return pd.read_csv(file_path, encoding='utf-8-sig')
            except Exception as e:
                print(f"âŒ åŠ è½½ {stock_code} æ•°æ®å¤±è´¥: {e}")
                return None
        return None
    
    def download_stock_batch(self, stock_codes, start_date='20200101', end_date='20241231', 
                           max_workers=5, batch_size=20):
        """æ‰¹é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®"""
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(stock_codes)} åªè‚¡ç¥¨æ•°æ®")
        print(f"   æ—¶é—´èŒƒå›´: {start_date} - {end_date}")
        print(f"   å¹¶å‘æ•°: {max_workers}, æ‰¹æ¬¡å¤§å°: {batch_size}")
        print("="*60)
        
        results = {}
        failed_stocks = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(stock_codes), batch_size):
            batch = stock_codes[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(stock_codes) + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} åªè‚¡ç¥¨)")
            
            # å¤šçº¿ç¨‹ä¸‹è½½å½“å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_stock = {
                    executor.submit(self.download_single_stock, code, start_date, end_date): code 
                    for code in batch
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_stock):
                    stock_code = future_to_stock[future]
                    try:
                        result = future.result()
                        if result is not None:
                            results[stock_code] = result
                            print(f"   âœ… {stock_code}: {len(result)} æ¡è®°å½•")
                        else:
                            failed_stocks.append(stock_code)
                            print(f"   âŒ {stock_code}: ä¸‹è½½å¤±è´¥")
                    except Exception as e:
                        failed_stocks.append(stock_code)
                        print(f"   âŒ {stock_code}: å¼‚å¸¸ - {e}")
            
            # ä¿å­˜ç¼“å­˜çŠ¶æ€
            self.save_cache_status()
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if i + batch_size < len(stock_codes):
                print(f"   â¸ï¸  æ‰¹æ¬¡ä¼‘æ¯ 2 ç§’...")
                time.sleep(2)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = len(results)
        fail_count = len(failed_stocks)
        
        print("\n" + "="*60)
        print(f"ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡:")
        print(f"   æˆåŠŸ: {success_count} åª")
        print(f"   å¤±è´¥: {fail_count} åª")
        print(f"   æˆåŠŸç‡: {success_count/(success_count+fail_count)*100:.1f}%")
        
        if failed_stocks:
            print(f"\nâŒ å¤±è´¥è‚¡ç¥¨åˆ—è¡¨: {failed_stocks[:10]}")
            if len(failed_stocks) > 10:
                print(f"   ... å¦å¤– {len(failed_stocks)-10} åª")
        
        return results, failed_stocks
    
    def load_stock_universe(self, universe_file='stock_universe.csv'):
        """åŠ è½½è‚¡ç¥¨æ± """
        try:
            # å°†codeåˆ—ä½œä¸ºå­—ç¬¦ä¸²è¯»å–ï¼Œä¿æŒå‰å¯¼é›¶
            universe_df = pd.read_csv(universe_file, encoding='utf-8-sig', dtype={'code': str})
            
            # ç¡®ä¿è‚¡ç¥¨ä»£ç æ ¼å¼æ­£ç¡®ï¼ˆ6ä½æ•°å­—ï¼Œä¸è¶³è¡¥é›¶ï¼‰
            universe_df['code'] = universe_df['code'].str.zfill(6)
            
            stock_codes = universe_df['code'].tolist()
            print(f"ğŸ“‹ ä» {universe_file} åŠ è½½ {len(stock_codes)} åªè‚¡ç¥¨")
            print(f"   ç¤ºä¾‹ä»£ç : {stock_codes[:5]}")
            return stock_codes
        except Exception as e:
            print(f"âŒ åŠ è½½è‚¡ç¥¨æ± å¤±è´¥: {e}")
            return []
    
    def validate_data_quality(self, stock_codes):
        """éªŒè¯æ•°æ®è´¨é‡"""
        print("\nğŸ” éªŒè¯æ•°æ®è´¨é‡...")
        
        quality_report = {
            'total_stocks': len(stock_codes),
            'valid_stocks': 0,
            'insufficient_data': [],
            'missing_files': [],
            'date_range_stats': {}
        }
        
        for code in stock_codes:
            data = self.load_stock_data(code)
            if data is None:
                quality_report['missing_files'].append(code)
                continue
            
            if len(data) < 1000:  # æé«˜æ•°æ®é‡è¦æ±‚è‡³1000ä¸ªäº¤æ˜“æ—¥
                quality_report['insufficient_data'].append(code)
                continue
            
            quality_report['valid_stocks'] += 1
            
            # ç»Ÿè®¡æ—¥æœŸèŒƒå›´
            min_date = data['date'].min()
            max_date = data['date'].max()
            quality_report['date_range_stats'][code] = {
                'start_date': str(min_date),
                'end_date': str(max_date),
                'trading_days': len(data)
            }
        
        # è¾“å‡ºè´¨é‡æŠ¥å‘Š
        print(f"   ğŸ“Š è´¨é‡ç»Ÿè®¡:")
        print(f"   - æ€»è‚¡ç¥¨æ•°: {quality_report['total_stocks']}")
        print(f"   - æ•°æ®æœ‰æ•ˆ: {quality_report['valid_stocks']}")
        print(f"   - æ•°æ®ä¸è¶³: {len(quality_report['insufficient_data'])}")
        print(f"   - æ–‡ä»¶ç¼ºå¤±: {len(quality_report['missing_files'])}")
        
        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        report_file = self.cache_dir / 'data_quality_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(quality_report, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ’¾ è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return quality_report
    
    def validate_liquidity_requirements(self, stock_codes, min_trading_days=200, 
                                      lookback_days=250):
        """éªŒè¯æµåŠ¨æ€§è¦æ±‚ - æ–°å¢åŠŸèƒ½"""
        print(f"\nğŸ’§ éªŒè¯æµåŠ¨æ€§è¦æ±‚ (è¿‘{lookback_days}å¤©ä¸­â‰¥{min_trading_days}å¤©æœ‰äº¤æ˜“)...")
        
        liquidity_report = {
            'total_stocks': len(stock_codes),
            'liquid_stocks': [],
            'illiquid_stocks': [],
            'missing_data': []
        }
        
        # è®¡ç®—æˆªæ­¢æ—¥æœŸ (è¿‘250ä¸ªè‡ªç„¶æ—¥å¯¹åº”çº¦200ä¸ªäº¤æ˜“æ—¥)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days)
        
        for code in stock_codes:
            data = self.load_stock_data(code)
            if data is None:
                liquidity_report['missing_data'].append(code)
                continue
            
            # è½¬æ¢æ—¥æœŸåˆ—
            data['date'] = pd.to_datetime(data['date'])
            
            # ç­›é€‰è¿‘æœŸæ•°æ®
            recent_data = data[data['date'] >= start_date]
            
            if len(recent_data) >= min_trading_days:
                liquidity_report['liquid_stocks'].append({
                    'code': code,
                    'trading_days': len(recent_data)
                })
            else:
                liquidity_report['illiquid_stocks'].append({
                    'code': code,
                    'trading_days': len(recent_data)
                })
        
        print(f"   ğŸ“Š æµåŠ¨æ€§ç»Ÿè®¡:")
        print(f"   - æ€»è‚¡ç¥¨æ•°: {liquidity_report['total_stocks']}")
        print(f"   - æµåŠ¨æ€§å……è¶³: {len(liquidity_report['liquid_stocks'])}")
        print(f"   - æµåŠ¨æ€§ä¸è¶³: {len(liquidity_report['illiquid_stocks'])}")
        print(f"   - æ•°æ®ç¼ºå¤±: {len(liquidity_report['missing_data'])}")
        
        # ä¿å­˜æµåŠ¨æ€§æŠ¥å‘Š
        liquidity_file = self.cache_dir / 'liquidity_report.json'
        with open(liquidity_file, 'w', encoding='utf-8') as f:
            json.dump(liquidity_report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"   ğŸ’¾ æµåŠ¨æ€§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {liquidity_file}")
        
        return liquidity_report
    
    def collect_market_data_batch(self, stock_codes):
        """æ‰¹é‡è·å–å¸‚åœºæ•°æ® (å¸‚å€¼ã€PEã€PBç­‰)"""
        print("\nğŸ“ˆ è·å–å¸‚åœºæ•°æ®...")
        
        try:
            # è·å–å®æ—¶è¡Œæƒ…æ•°æ®
            market_data = ak.stock_zh_a_spot_em()
            
            # ç­›é€‰ç›®æ ‡è‚¡ç¥¨
            target_data = market_data[market_data['ä»£ç '].isin(stock_codes)].copy()
            
            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'ä»£ç ': 'code',
                'åç§°': 'name',
                'æ€»å¸‚å€¼': 'total_market_cap',
                'æµé€šå¸‚å€¼': 'float_market_cap',
                'å¸‚ç›ˆç‡-åŠ¨æ€': 'pe_ttm',
                'å¸‚å‡€ç‡': 'pb'
            }
            
            target_data = target_data.rename(columns=column_mapping)
            
            # é€‰æ‹©éœ€è¦çš„åˆ—
            keep_columns = ['code', 'name', 'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb']
            target_data = target_data[keep_columns]
            
            # ä¿å­˜å¸‚åœºæ•°æ®
            market_file = self.data_dir / 'market_data.csv'
            target_data.to_csv(market_file, index=False, encoding='utf-8-sig')
            
            print(f"   âœ… æˆåŠŸè·å– {len(target_data)} åªè‚¡ç¥¨çš„å¸‚åœºæ•°æ®")
            print(f"   ğŸ’¾ å·²ä¿å­˜åˆ°: {market_file}")
            
            return target_data
            
        except Exception as e:
            print(f"   âŒ è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return None

def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œæ‰©å±•æ•°æ®é‡‡é›† (Phase 1.2)"""
    import argparse
    parser = argparse.ArgumentParser(description='æ‰©å±•æ•°æ®é‡‡é›†')
    parser.add_argument('--universe', type=str, default='stock_universe.csv')
    parser.add_argument('--start', type=str, default='20200101')
    parser.add_argument('--end', type=str, default='20241231')
    parser.add_argument('--max-workers', type=int, default=5)
    parser.add_argument('--batch-size', type=int, default=20)
    args = parser.parse_args()

    print("ğŸš€ å¯åŠ¨Phase 1.2: æ‰©å±•æ•°æ®é‡‡é›†")
    print("="*70)
    print(f"ğŸ¯ è‚¡ç¥¨æ± : {args.universe}")
    print(f"ğŸ—“ï¸ æ—¶é—´èŒƒå›´: {args.start} ~ {args.end}")
    
    collector = DataCollector()
    
    # 1. åŠ è½½æ‰©å±•è‚¡ç¥¨æ± 
    stock_codes = collector.load_stock_universe(args.universe)
    if not stock_codes:
        print("âŒ æ— æ³•åŠ è½½è‚¡ç¥¨æ± ")
        return
    
    # 2. æ‰¹é‡ä¸‹è½½5å¹´å†å²æ•°æ® (å‡çº§å‚æ•°)
    print(f"\nğŸ¯ å¼€å§‹ä¸‹è½½ {len(stock_codes)} åªè‚¡ç¥¨çš„æ•°æ®...")
    results, failed_stocks = collector.download_stock_batch(
        stock_codes=stock_codes,
        start_date=args.start,
        end_date=args.end,
        max_workers=args.max_workers,
        batch_size=args.batch_size
    )
    
    # 3. è·å–å¸‚åœºæ•°æ®
    market_data = collector.collect_market_data_batch(stock_codes)
    
    # 4. æ•°æ®è´¨é‡éªŒè¯ (æ›´ä¸¥æ ¼æ ‡å‡†)
    quality_report = collector.validate_data_quality(stock_codes)
    
    # 5. æ–°å¢ï¼šæµåŠ¨æ€§éªŒè¯
    liquidity_report = collector.validate_liquidity_requirements(stock_codes)
    
    # 6. è‡ªåŠ¨é‡è¯•å¤±è´¥è‚¡ç¥¨
    if failed_stocks:
        print(f"\nğŸ”„ è‡ªåŠ¨é‡è¯• {len(failed_stocks)} åªå¤±è´¥è‚¡ç¥¨...")
        retry_results, still_failed = collector.download_stock_batch(
            stock_codes=failed_stocks,
            start_date=args.start,
            end_date=args.end,
            max_workers=2,
            batch_size=5
        )
        print(f"   é‡è¯•æˆåŠŸ: {len(retry_results)} åª")
        print(f"   ä»ç„¶å¤±è´¥: {len(still_failed)} åª")
    
    # 7. Phase 1.2 æœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*70)
    print("âœ… Phase 1.2 å®Œæˆ: æ‰©å±•æ•°æ®é‡‡é›†!")
    print(f"ğŸ“Š é‡‡é›†ç»Ÿè®¡:")
    print(f"   - ç›®æ ‡è‚¡ç¥¨: {len(stock_codes)} åª")
    print(f"   - æˆåŠŸä¸‹è½½: {len(results)} åª")
    print(f"   - æ•°æ®æœ‰æ•ˆ: {quality_report['valid_stocks']} åª (â‰¥1000äº¤æ˜“æ—¥)")
    print(f"   - æµåŠ¨æ€§å……è¶³: {len(liquidity_report['liquid_stocks'])} åª")
    print(f"   - æ—¶é—´èŒƒå›´: 2020-2024 (5å¹´)")
    print(f"   - é¢„æœŸè§‚æµ‹é‡: 375k+ (vs åŸ47k)")
    
    if still_failed if 'still_failed' in locals() else failed_stocks:
        remaining_failed = still_failed if 'still_failed' in locals() else failed_stocks
        print(f"   âš ï¸ ä»éœ€å…³æ³¨: {len(remaining_failed)} åªè‚¡ç¥¨æ•°æ®è·å–å¤±è´¥")
    
    print("âœ… ä¸‹ä¸€æ­¥: Phase 1.3 ç²¾ç®€å› å­è‡³6ä¸ªæ ¸å¿ƒå› å­")

if __name__ == "__main__":
    main()
