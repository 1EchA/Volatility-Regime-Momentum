#!/usr/bin/env python3
"""
Aè‚¡æ•°æ®é‡‡é›†æ¨¡å— - 50åªè‚¡ç¥¨æ‰©å±•ç‰ˆæœ¬
æ”¯æŒ7å¹´å†å²æ•°æ®ã€50åªç²¾é€‰è‚¡ç¥¨ã€è´¨é‡ä¼˜å…ˆçš„æ•°æ®æ”¶é›†

é‡è¦æ›´æ–°:
- æ—¶é—´èŒƒå›´: 2018-2024 (7å¹´)
- è‚¡ç¥¨æ•°é‡: 50åªç²¾é€‰è‚¡ç¥¨ (ç§‘å­¦åˆ†å±‚æŠ½æ ·)
- è´¨é‡æ§åˆ¶: æ›´ä¸¥æ ¼çš„æ•°æ®éªŒè¯å’Œå®Œæ•´æ€§æ£€æŸ¥
- æ€§èƒ½ä¼˜åŒ–: é€‚é…å°è§„æ¨¡é«˜è´¨é‡é‡‡é›†
"""

import akshare as ak
import pandas as pd
import numpy as np
import os
import time
import json
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class DataCollector:
    def __init__(self, data_dir='data', cache_dir='cache'):
        self.data_dir = Path(data_dir)
        self.cache_dir = Path(cache_dir)

        # åˆ›å»ºç›®å½•
        self.data_dir.mkdir(exist_ok=True)
        self.cache_dir.mkdir(exist_ok=True)

        # çº¿ç¨‹é”ï¼Œç”¨äºAPIé™é¢‘
        self.api_lock = threading.Lock()
        self.last_api_call = 0
        self.min_interval = 0.1  # APIè°ƒç”¨é—´éš”(ç§’)

        # ç¼“å­˜é…ç½®
        self.cache_file = self.cache_dir / 'download_status.json'
        self.load_cache_status()

        # 50åªç²¾é€‰è‚¡ç¥¨ä»£ç 
        self.SELECTED_STOCKS = [
            # è¶…å¤§ç›˜è‚¡ (10åª)
            '601138', '688981', '000333', '601601', '002415',
            '000651', '601766', '302132', '300033', '601881',

            # å¤§ç›˜è‚¡ (12åª)
            '600031', '605499', '300014', '601888', '601800',
            '000617', '601390', '000166', '600989', '600905',
            '600886', '600415',

            # ä¸­å¤§ç›˜è‚¡ (15åª)
            '000895', '001979', '600703', '002466', '000807',
            '601872', '601607', '002648', '300251', '600426',
            '002202', '300207', '601058', '000999', '002555',

            # ä¸­ç›˜è‚¡ (10åª)
            '300316', '002252', '601598', '002601', '603087',
            '000786', '603129', '002078', '002032', '600392'
        ]

        print(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"ğŸ“Š è‚¡ç¥¨æ•°é‡: {len(self.SELECTED_STOCKS)} åª")

    def load_cache_status(self):
        """åŠ è½½ä¸‹è½½çŠ¶æ€ç¼“å­˜"""
        if self.cache_file.exists():
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                self.cache_status = json.load(f)
        else:
            self.cache_status = {}

        print(f"ğŸ“‹ å·²åŠ è½½ç¼“å­˜çŠ¶æ€: {len(self.cache_status)} æ¡è®°å½•")

    def save_cache_status(self):
        """ä¿å­˜ä¸‹è½½çŠ¶æ€ç¼“å­˜"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache_status, f, ensure_ascii=False, indent=2)

    def rate_limit(self):
        """APIè°ƒç”¨é™é¢‘æ§åˆ¶"""
        with self.api_lock:
            current_time = time.time()
            time_since_last = current_time - self.last_api_call
            if time_since_last < self.min_interval:
                time.sleep(self.min_interval - time_since_last)
            self.last_api_call = time.time()

    def download_single_stock(self, stock_code, start_date='20180101', end_date='20241231'):
        """ä¸‹è½½å•åªè‚¡ç¥¨çš„7å¹´å†å²æ•°æ®"""

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{stock_code}_{start_date}_{end_date}"
        if cache_key in self.cache_status and self.cache_status[cache_key]['status'] == 'success':
            return self.load_stock_data(stock_code)

        try:
            # APIé™é¢‘
            self.rate_limit()

            print(f"   ğŸ“¥ æ­£åœ¨ä¸‹è½½ {stock_code} æ•°æ®...")

            # è·å–æ—¥åº¦è¡Œæƒ…æ•°æ®
            daily_data = ak.stock_zh_a_hist(
                symbol=stock_code,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust=""  # ä¸å¤æƒï¼Œåç»­è‡ªå·±å¤„ç†
            )

            if daily_data.empty:
                raise ValueError(f"è‚¡ç¥¨ {stock_code} æ— æ•°æ®")

            # æ•°æ®é¢„å¤„ç†
            daily_data = self.preprocess_daily_data(daily_data, stock_code)

            # æ•°æ®è´¨é‡æ£€æŸ¥
            self.validate_single_stock_data(daily_data, stock_code)

            # ä¿å­˜æ•°æ®
            output_file = self.data_dir / f"{stock_code}.csv"
            daily_data.to_csv(output_file, index=False, encoding='utf-8-sig')

            # æ›´æ–°ç¼“å­˜çŠ¶æ€
            self.cache_status[cache_key] = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'rows': len(daily_data),
                'date_range': f"{daily_data['date'].min()} - {daily_data['date'].max()}"
            }

            print(f"   âœ… {stock_code} ä¸‹è½½æˆåŠŸ: {len(daily_data)} æ¡è®°å½•")
            return daily_data

        except Exception as e:
            error_msg = f"ä¸‹è½½ {stock_code} å¤±è´¥: {str(e)}"
            print(f"   âŒ {error_msg}")

            # è®°å½•å¤±è´¥çŠ¶æ€
            self.cache_status[cache_key] = {
                'status': 'failed',
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }

            return None

    def preprocess_daily_data(self, data, stock_code):
        """é¢„å¤„ç†æ—¥åº¦æ•°æ®"""
        # é‡å‘½ååˆ—åä¸ºè‹±æ–‡
        column_mapping = {
            'æ—¥æœŸ': 'date',
            'å¼€ç›˜': 'open',
            'æ”¶ç›˜': 'close',
            'æœ€é«˜': 'high',
            'æœ€ä½': 'low',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'amount',
            'æŒ¯å¹…': 'amplitude',
            'æ¶¨è·Œå¹…': 'pct_change',
            'æ¶¨è·Œé¢': 'change',
            'æ¢æ‰‹ç‡': 'turnover_rate'
        }

        # åº”ç”¨åˆ—åæ˜ å°„
        for old_name, new_name in column_mapping.items():
            if old_name in data.columns:
                data = data.rename(columns={old_name: new_name})

        # æ·»åŠ è‚¡ç¥¨ä»£ç 
        data['stock_code'] = stock_code

        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeæ ¼å¼
        data['date'] = pd.to_datetime(data['date'])

        # æŒ‰æ—¥æœŸæ’åº
        data = data.sort_values('date').reset_index(drop=True)

        # æ•°æ®ç±»å‹è½¬æ¢
        numeric_columns = ['open', 'close', 'high', 'low', 'volume', 'amount']
        for col in numeric_columns:
            if col in data.columns:
                data[col] = pd.to_numeric(data[col], errors='coerce')

        return data

    def validate_single_stock_data(self, data, stock_code):
        """éªŒè¯å•åªè‚¡ç¥¨æ•°æ®è´¨é‡"""
        if len(data) < 1200:  # 7å¹´çº¦1750ä¸ªäº¤æ˜“æ—¥ï¼Œè¦æ±‚è‡³å°‘1200å¤©
            raise ValueError(f"æ•°æ®é‡ä¸è¶³: {len(data)} < 1200 å¤©")

        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['date', 'open', 'close', 'high', 'low', 'volume']
        missing_fields = [field for field in required_fields if field not in data.columns]
        if missing_fields:
            raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")

        # æ£€æŸ¥ç©ºå€¼
        for field in required_fields:
            null_count = data[field].isnull().sum()
            if null_count > len(data) * 0.05:  # ç©ºå€¼ä¸è¶…è¿‡5%
                raise ValueError(f"å­—æ®µ {field} ç©ºå€¼è¿‡å¤š: {null_count}/{len(data)}")

    def load_stock_data(self, stock_code):
        """åŠ è½½å•åªè‚¡ç¥¨æ•°æ®"""
        try:
            file_path = self.data_dir / f"{stock_code}.csv"
            if file_path.exists():
                data = pd.read_csv(file_path, encoding='utf-8-sig')
                data['date'] = pd.to_datetime(data['date'])
                return data
            else:
                return None
        except Exception as e:
            print(f"âŒ åŠ è½½ {stock_code} æ•°æ®å¤±è´¥: {e}")
            return None

    def download_all_stocks(self, start_date='20180101', end_date='20241231', max_workers=3):
        """ä¸‹è½½æ‰€æœ‰50åªè‚¡ç¥¨æ•°æ®"""

        print(f"\nğŸš€ å¼€å§‹ä¸‹è½½ {len(self.SELECTED_STOCKS)} åªè‚¡ç¥¨çš„7å¹´å†å²æ•°æ®")
        print(f"   æ—¶é—´èŒƒå›´: {start_date} - {end_date}")
        print(f"   å¹¶å‘æ•°: {max_workers}")
        print("="*60)

        successful_downloads = []
        failed_downloads = []

        # ä½¿ç”¨çº¿ç¨‹æ± ä¸‹è½½
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_code = {
                executor.submit(self.download_single_stock, code, start_date, end_date): code
                for code in self.SELECTED_STOCKS
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_code):
                stock_code = future_to_code[future]
                try:
                    result = future.result()
                    if result is not None:
                        successful_downloads.append(stock_code)
                    else:
                        failed_downloads.append(stock_code)
                except Exception as e:
                    print(f"   âŒ {stock_code} çº¿ç¨‹å¼‚å¸¸: {e}")
                    failed_downloads.append(stock_code)

        # ä¿å­˜ç¼“å­˜çŠ¶æ€
        self.save_cache_status()

        # è¾“å‡ºç»“æœç»Ÿè®¡
        print("\n" + "="*60)
        print(f"ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸ: {len(successful_downloads)} åª")
        print(f"   âŒ å¤±è´¥: {len(failed_downloads)} åª")

        if failed_downloads:
            print(f"   å¤±è´¥è‚¡ç¥¨: {failed_downloads}")

        return successful_downloads, failed_downloads

    def validate_data_quality(self, stock_codes=None):
        """éªŒè¯æ•°æ®è´¨é‡"""
        if stock_codes is None:
            stock_codes = self.SELECTED_STOCKS

        print("\nğŸ” éªŒè¯æ•°æ®è´¨é‡...")

        quality_report = {
            'total_stocks': len(stock_codes),
            'valid_stocks': 0,
            'insufficient_data': [],
            'missing_files': [],
            'date_range_stats': {},
            'quality_score': 0
        }

        for code in stock_codes:
            data = self.load_stock_data(code)
            if data is None:
                quality_report['missing_files'].append(code)
                continue

            if len(data) < 1200:  # 7å¹´æ•°æ®è¦æ±‚è‡³å°‘1200ä¸ªäº¤æ˜“æ—¥
                quality_report['insufficient_data'].append(code)
                continue

            quality_report['valid_stocks'] += 1

            # ç»Ÿè®¡æ—¥æœŸèŒƒå›´
            min_date = data['date'].min()
            max_date = data['date'].max()
            quality_report['date_range_stats'][code] = {
                'start_date': str(min_date),
                'end_date': str(max_date),
                'trading_days': len(data),
                'completeness': len(data) / 1750  # 7å¹´ç†è®ºäº¤æ˜“æ—¥æ•°
            }

        # è®¡ç®—è´¨é‡è¯„åˆ†
        quality_report['quality_score'] = quality_report['valid_stocks'] / quality_report['total_stocks']

        # è¾“å‡ºè´¨é‡æŠ¥å‘Š
        print(f"   ğŸ“Š è´¨é‡ç»Ÿè®¡:")
        print(f"   - æ€»è‚¡ç¥¨æ•°: {quality_report['total_stocks']}")
        print(f"   - æ•°æ®æœ‰æ•ˆ: {quality_report['valid_stocks']}")
        print(f"   - æ•°æ®ç¼ºå¤±: {len(quality_report['missing_files'])}")
        print(f"   - æ•°æ®ä¸è¶³: {len(quality_report['insufficient_data'])}")
        print(f"   - è´¨é‡è¯„åˆ†: {quality_report['quality_score']:.1%}")

        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        report_file = self.data_dir / 'data_quality_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(quality_report, f, ensure_ascii=False, indent=2, default=str)

        print(f"   ğŸ’¾ è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        return quality_report

    def generate_summary_statistics(self):
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        print("\nğŸ“ˆ ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡...")

        all_data = []
        for code in self.SELECTED_STOCKS:
            data = self.load_stock_data(code)
            if data is not None:
                all_data.append(data)

        if not all_data:
            print("   âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_data = pd.concat(all_data, ignore_index=True)

        # è®¡ç®—åŸºç¡€ç»Ÿè®¡
        stats = {
            'total_observations': len(combined_data),
            'total_stocks': len(all_data),
            'date_range': {
                'start': str(combined_data['date'].min()),
                'end': str(combined_data['date'].max())
            },
            'avg_trading_days_per_stock': len(combined_data) / len(all_data),
            'price_statistics': {
                'avg_price': combined_data['close'].mean(),
                'median_price': combined_data['close'].median(),
                'price_std': combined_data['close'].std()
            }
        }

        print(f"   ğŸ“Š æ•°æ®æ±‡æ€»:")
        print(f"   - æ€»è§‚æµ‹å€¼: {stats['total_observations']:,}")
        print(f"   - æœ‰æ•ˆè‚¡ç¥¨: {stats['total_stocks']}")
        print(f"   - æ—¥æœŸèŒƒå›´: {stats['date_range']['start']} ~ {stats['date_range']['end']}")
        print(f"   - å¹³å‡äº¤æ˜“æ—¥/è‚¡ç¥¨: {stats['avg_trading_days_per_stock']:.0f}")

        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        stats_file = self.data_dir / 'summary_statistics.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2, default=str)

        print(f"   ğŸ’¾ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {stats_file}")

        return stats

def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œ50åªè‚¡ç¥¨7å¹´æ•°æ®é‡‡é›†"""

    import argparse
    parser = argparse.ArgumentParser(description='50åªè‚¡ç¥¨æ•°æ®é‡‡é›†')
    parser.add_argument('--start', type=str, default='20180101')
    parser.add_argument('--end', type=str, default='20241231')
    parser.add_argument('--max-workers', type=int, default=3)
    args = parser.parse_args()

    print("ğŸš€ å¯åŠ¨50åªè‚¡ç¥¨æ‰©å±•æ•°æ®é‡‡é›†")
    print("="*70)
    print("ğŸ“Š æ ·æœ¬è®¾è®¡: ç§‘å­¦åˆ†å±‚æŠ½æ ·ï¼Œå…¼é¡¾ä»£è¡¨æ€§ä¸è´¨é‡")
    print(f"ğŸ—“ï¸ æ—¶é—´èŒƒå›´: {args.start} ~ {args.end}")

    collector = DataCollector()

    # 1. ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    print(f"\nğŸ¯ å¼€å§‹ä¸‹è½½ {len(collector.SELECTED_STOCKS)} åªè‚¡ç¥¨çš„æ•°æ®...")
    successful_stocks, failed_stocks = collector.download_all_stocks(
        start_date=args.start,
        end_date=args.end,
        max_workers=args.max_workers
    )

    # 2. æ•°æ®è´¨é‡éªŒè¯
    quality_report = collector.validate_data_quality()

    # 3. ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    summary_stats = collector.generate_summary_statistics()

    # 4. é‡è¯•å¤±è´¥è‚¡ç¥¨ï¼ˆå¦‚æœæœ‰ï¼‰
    if failed_stocks:
        print(f"\nğŸ”„ é‡è¯• {len(failed_stocks)} åªå¤±è´¥è‚¡ç¥¨...")
        retry_successful, still_failed = collector.download_all_stocks(
            start_date='20180101',
            end_date='20241231',
            max_workers=1  # å•çº¿ç¨‹é‡è¯•
        )

        if still_failed:
            print(f"   âš ï¸ ä»æœ‰ {len(still_failed)} åªè‚¡ç¥¨ä¸‹è½½å¤±è´¥: {still_failed}")
        else:
            print("   âœ… æ‰€æœ‰è‚¡ç¥¨é‡è¯•æˆåŠŸ!")

    # 5. æœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*70)
    print("ğŸ‰ 50åªè‚¡ç¥¨æ•°æ®é‡‡é›†å®Œæˆ!")
    print(f"âœ… æˆåŠŸç‡: {quality_report['quality_score']:.1%}")

    if quality_report['quality_score'] >= 0.9:
        print("ğŸ† æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥åˆ†æ!")
    elif quality_report['quality_score'] >= 0.8:
        print("ğŸ‘ æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®æ£€æŸ¥ç¼ºå¤±æ•°æ®åç»§ç»­!")
    else:
        print("âš ï¸ æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œå’ŒAPIçŠ¶æ€!")

    print("="*70)

if __name__ == "__main__":
    main()
